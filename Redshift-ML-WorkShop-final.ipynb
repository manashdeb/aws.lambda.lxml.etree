{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Redshift ML Workshop \n",
    "---\n",
    "\n",
    "### Overview of the Workshop\n",
    "All labs in this workshop use Jupyter notebooks running on Amazon SageMaker Notebook Instances. Three Labs will be covered in this workshop based upon three user personas.   \n",
    "1. **Data Analyst - Naive Machine Learning User**  \n",
    "2. **Advanced Data Analyst - Intermediate Machine Learning User**  \n",
    "3. **Data Sciencist -  Advanced Machine Expert**  \n",
    "\n",
    "\n",
    "### Lab Components\n",
    "    \n",
    "* __Jupyter Notebook__:  \n",
    "You are currently in a Jupyter notebook. This is an exploratory environment where you can un many different types of code, see the results, and interact them. Each of the 4 labs in this workshop is a single notebook. These notebooks are accessible through the table of contents at the top of any lab.\n",
    "\n",
    "* __Amazon SageMaker Notebook Instance__:  \n",
    "This notebook is running in an Amazon SageMaker notebook instance. This is a fully managed Amazon EC2 instance that has a preconfigured Jupyter notebook server and a set of `conda` libraries. All necessary dependencies for the labs in this workshop are already present. \n",
    "\n",
    "* __`conda` Python Kernel__:  \n",
    "Kernels are processes that receive and execute interactive code and return output to the user. The notebook frontend communicates with the kernel backend. In these labs we use the `conda_python3` kernel.\n",
    "\n",
    "[Project Jupyter]: https://jupyter.org/\n",
    "[SageMaker example notebooks]: https://github.com/awslabs/amazon-sagemaker-examples\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tips:\n",
    " \n",
    "* Labs progress by running the grey `code` cells _in order_ top to bottom.\n",
    "* Each cell has a title text to explain what happens when you run it.\n",
    "* Chrome is recommended but any modern browser should work\n",
    "* Poor network connectivity may cause minor delays when navigating the notebook.\n",
    "* When a cell is running you will see the text to the left change to `In [*]:`.\n",
    "* When a cell's code has finished you will see the text to the left change to `In [19]:`. \n",
    "    * The number indicates the order in which the cell was run.\n",
    "* We're here to help if you get stuck or something doesn't work please let us know.\n",
    "* **Finally** - You're free to experiment and rerun cells. \n",
    "    * Nothing should break if cells are run more than once or out of order or rerun."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Customize Labs notebooks for your test account\n",
    "Setup credentials to access the Redshift cluster.\n",
    "In this step, please replace the `host_name` with your Redshift cluster's `hostname`. \n",
    "\n",
    "We will also install some python libaries needed for this notebook.\n",
    "\n",
    "-----\n",
    "**Expected Outputs**: None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "\n",
    "echo \"{\n",
    "  \\\"user_name\\\": \\\"awsuser\\\",\n",
    "  \\\"password\\\": \\\"Password123\\\",\n",
    "  \\\"host_name\\\": \\\"redshift-demo-ml.ch4d7ft4m5gg.us-east-1.redshift.amazonaws.com\\\",\n",
    "  \\\"port_num\\\": \\\"5439\\\",\n",
    "  \\\"db_name\\\": \\\"dev\\\"\n",
    "}\" > redshift-ml-workshop.creds\n",
    "\n",
    "cat redshift-ml-workshop.creds\n",
    "\n",
    "pip install psycopg2-binary\n",
    "pip install sqlalchemy \n",
    "pip install simplejson\n",
    "pip install ipython-sql\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Connect to your Redshift cluster and run a query\n",
    "You will use the sqlalchemy and ipython-sql Python libraries to manage the Redshift connection.  \n",
    "This test confirms that you can proceed with the rest of the Labs.\n",
    "\n",
    "-----\n",
    "**Sample Outputs**:\n",
    "* `'Connected: awsuser@dev'`\n",
    "* `current_user | version`\n",
    "* `awsuser\t   | PostgreSQL 8.0.2 on i686-pc-linux-gnu, compiled by GCC gcc (GCC) 3.4.2 20041017 (Red Hat 3.4.2-6.fc3), Redshift 1.0.11420`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlalchemy\n",
    "import psycopg2\n",
    "import simplejson\n",
    "\n",
    "\n",
    "from sqlalchemy import  create_engine\n",
    "\n",
    "\n",
    "#engine = create_engine(\"postgresql+psycopg2://awsuser:Password123@redshift-demo-ml.ch4d7ft4m5gg.us-east-1.redshift.amazonaws.com:5439/dev\")\n",
    "\n",
    "with open(\"redshift-ml-workshop.creds\") as fh:\n",
    "    creds = simplejson.loads(fh.read())\n",
    "connect_to_db = \"postgresql+psycopg2://\" + \\\n",
    "                creds[\"user_name\"] +':'+ creds[\"password\"] +'@'+ \\\n",
    "                creds[\"host_name\"] +':'+ creds[\"port_num\"] +'/'+ \"dev\";\n",
    "\n",
    "\n",
    "#engine = create_engine(\"postgresql+psycopg2://)\n",
    "engine = create_engine(connect_to_db)\n",
    "engine.connect() # connect to the database\n",
    "\n",
    "print(engine)\n",
    "\n",
    "#%reload_ext sql\n",
    "\n",
    "#%sql postgresql+psycopg2://awsuser:Password123@redshift-demo-ml.ch4d7ft4m5gg.us-east-1.redshift.amazonaws.com:5439/dev\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%reload_ext sql\n",
    "\n",
    "%sql SELECT current_user, version();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Redshift-ML-Workshop - Usecase 1 - Data Analyst User \n",
    "---\n",
    "\n",
    "### Data Set Information: ###\n",
    "### Bank Marketing data set ###\n",
    "\n",
    "The data is related with direct marketing campaigns of a Portuguese banking institution. The marketing campaigns were based on phone calls. Often, more than one contact to the same client was required, in order to access if the product (bank term deposit) would be ('yes') or not ('no') subscribed.\n",
    "\n",
    "\n",
    "The classification goal is to predict if the client will subscribe (yes/no) a term deposit (variable y).\n",
    "\n",
    "### Attribute Information /Input variables / bank client data: ###\n",
    "1 - age (numeric)   \n",
    "2 - job  \n",
    "3 - marital   \n",
    "4 - education   \n",
    "5 - default  \n",
    "6 - housing  \n",
    "7 - loan  \n",
    "8 - contact \n",
    "9 - month  \n",
    "10 - day_of_week  \n",
    "11 - duration  \n",
    "12 - campaign  \n",
    "13 - pdays  \n",
    "14 - previous  \n",
    "15 - poutcome  \n",
    "16 - emp.var.rate  \n",
    "17 - cons.price.idx  \n",
    "18 - cons.conf.idx  \n",
    "19 - euribor3m  \n",
    "20 - nr.employed  \n",
    "\n",
    "Output variable (desired target):  \n",
    "21 - y - has the client subscribed a term deposit? (binary: 'yes','no')  \n",
    "\n",
    "\n",
    "**Reference:** https://archive.ics.uci.edu/ml/datasets/bank+marketing\n",
    "\n",
    "Complete SQL is here <add the file>\n",
    "    \n",
    "Sample dataset is already loaded into `bank_details_training` and `bank_details_inference` tables. \n",
    "\n",
    "The create model with default run time (90 mins ) is already pre built in your Redshift cluster. In this workshop we will run the modified version of the create model with `MAX_RUNTIME` option set to a `900 secs`  keeping in mind the live session. \n",
    "\n",
    "For the inference queries we can use SQL function created by the prebuilt model .\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modified CREATE MODEL for Use Case 1 - Data Analyst User"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%sql\n",
    "/* -- Create model for bank marketing use case with max runtime 900 secs -- */\n",
    " CREATE MODEL model_bank_marketing_v2\n",
    "FROM (\n",
    "    SELECT age\n",
    "        ,job\n",
    "        ,marital\n",
    "        ,education\n",
    "        ,\"default\"\n",
    "        ,housing\n",
    "        ,loan\n",
    "        ,contact\n",
    "        ,month\n",
    "        ,day_of_week\n",
    "        ,duration\n",
    "        ,campaign\n",
    "        ,pdays\n",
    "        ,previous\n",
    "        ,poutcome\n",
    "        ,emp_var_rate\n",
    "        ,cons_price_idx\n",
    "        ,cons_conf_idx\n",
    "        ,euribor3m\n",
    "        ,nr_employed\n",
    "        ,y\n",
    "    FROM bank_details_training\n",
    "    ) TARGET y \n",
    "FUNCTION func_model_bank_marketing_v2 \n",
    "IAM_ROLE '<< replace IAM role arn >>' \n",
    "SETTINGS(S3_BUCKET '<< replace S3 output bucket >>', MAX_RUNTIME 900);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Show MODEL for Use Case 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "/* -- Show all models -- */\n",
    " SHOW model ALL;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "/* -- Show prebuilt model for bank marketing  -- */\n",
    " SHOW model model_bank_marketing;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "/* -- Show model for bank marketing created during the workshop -- */\n",
    " SHOW model model_bank_marketing_v2;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check Inference/Accuracy of the model `model_bank_marketing` .\n",
    "This is where you run the query to check the accuracy of the models. We will use the function created by the pre built model for the inference and against the data set in inference table `bank_details_inference`. Please feel free to run the same against the function that was created by the model we created in the workshop. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "\n",
    "/* -- Check accuracy for bank marketing using prebuilt model function  -- */\n",
    " WITH infer_data\n",
    "AS (\n",
    "\tSELECT y AS actual\n",
    "\t\t,func_model_bank_marketing(age, job, marital, education, \"default\", housing, loan, contact, month, day_of_week, duration, campaign, pdays, previous, poutcome, emp_var_rate, cons_price_idx, cons_conf_idx, euribor3m, nr_employed) AS predicted\n",
    "\t\t,CASE \n",
    "\t\t\tWHEN actual = predicted\n",
    "\t\t\t\tTHEN 1::INT\n",
    "\t\t\tELSE 0::INT\n",
    "\t\t\tEND AS correct\n",
    "\tFROM bank_details_inference\n",
    "\t)\n",
    "\t,aggr_data\n",
    "AS (\n",
    "\tSELECT SUM(correct) AS num_correct\n",
    "\t\t,COUNT(*) AS total\n",
    "\tFROM infer_data\n",
    "\t)\n",
    "SELECT (num_correct::FLOAT / total::FLOAT) AS accuracy\n",
    "FROM aggr_data;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predict how many customers will subscribe for term deposit vs not subscribe\n",
    "We are running this query against the dataset in inference table `bank_details_inference`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sample output for prediction query\n",
    "\n",
    "```sql\n",
    "     deposit_prediction     | count\n",
    "----------------------------+-------\n",
    " Yes-will-do-a-term-deposit |  5362\n",
    " No-term-deposit            | 35826\n",
    "(2 rows)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql \n",
    "/* -- Predict whether the customer will do a term deposit or not  -- */\n",
    "WITH term_data AS ( SELECT func_model_bank_marketing( age,job,marital,education,\"default\",housing,loan,contact,month,day_of_week,duration,campaign,pdays,previous,poutcome,emp_var_rate,cons_price_idx,cons_conf_idx,euribor3m,nr_employed) AS predicted \n",
    "FROM bank_details_inference )\n",
    "SELECT \n",
    "CASE WHEN predicted = 'Y'  THEN 'Yes-will-do-a-term-deposit'\n",
    "     WHEN predicted = 'N'  THEN 'No-term-deposit'\n",
    "     ELSE 'Neither' END as deposit_prediction,\n",
    "COUNT(1) AS count\n",
    "from term_data GROUP BY 1;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Redshift-ML-Workshop - Usecase 2 - Advanced Data Analyst User \n",
    "----  \n",
    "\n",
    "### Data Set Information: ####\n",
    "### Iris data set ###\n",
    "\n",
    "This is perhaps the best known database to be found in the pattern recognition literature. Fisher's paper is a classic in the field and is referenced frequently to this day. (See Duda & Hart, for example.) \n",
    "The data set contains 3 classes of 50 instances each, where each class refers to a type of iris plant. One class is linearly separable from the other 2; the latter are NOT linearly separable from each other.\n",
    "\n",
    "*Predicted attribute:* class of iris plant.\n",
    "\n",
    "\n",
    "Attribute Information:\n",
    "\n",
    "1. sepal length in cm\n",
    "2. sepal width in cm\n",
    "3. petal length in cm\n",
    "4. petal width in cm\n",
    "5. class:\n",
    "-- Iris Setosa\n",
    "-- Iris Versicolour\n",
    "-- Iris Virginica */\n",
    "\n",
    "User creates model and supplies some information like the `PROBLEM_TYPE` and `OBJECTIVE` as part of the create model process. \n",
    "\n",
    "SageMaker Autopilot chooses the `PROBLEM_TYPE` and `OBJECTIVE` specified by the user instead of trying everything. \n",
    "\n",
    "For this example, we are going to provide the `PROBLEM_TYPE` which is  `multiclass classification`. \n",
    "\n",
    "For all problem_types supported by SageMaker Autopilot - https://docs.aws.amazon.com/sagemaker/latest/dg/autopilot-automate-model-development-problem-types.html \n",
    "\n",
    "The `OBJECTIVE` we are going to provide is  `accuracy`. The objective metric is used to measure the predictive quality of a machine learning system. \n",
    "\n",
    "*Default:* MSE: for regression, F1: for binary classification, Accuracy: for multiclass classification\n",
    "\n",
    "For all objectives supported refer: https://docs.aws.amazon.com/redshift/latest/dg/r_CREATE_MODEL.html#r_user_guidance_create_model\n",
    "\n",
    "Complete SQL is here <insert sql here > \n",
    "\n",
    "Sample dataset is already loaded into `iris_data_train` and `iris_data_test` tables.\n",
    "\n",
    "The create model with default run time (90 mins ) is already pre built in your Redshift cluster. For the workshop we will run the modified version of the create model with `MAX_RUNTIME` option set to a 900 secs. \n",
    "\n",
    "For the inference queries we can use SQL function created by the prebuilt model .\n",
    "\n",
    "Please make sure to change the `IAM role` and `S3 bucket`. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Model for Advanced Data Analyst User ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "/* -- Create model for iris use case - with max runtime 900 secs -- */\n",
    " CREATE MODEL model_iris_v2\n",
    "FROM (\n",
    "SELECT \n",
    "   Id,\n",
    "   SepalLengthCm,\n",
    "   SepalWidthCm,\n",
    "   PetalLengthCm,\n",
    "   PetalWidthCm,\n",
    "   Species\n",
    "FROM iris_data_train\n",
    ")\n",
    "TARGET Species \n",
    "FUNCTION func_model_iris_v2 IAM_ROLE '<< replace IAM role arn >>' \n",
    "PROBLEM_TYPE multiclass_classification \n",
    "OBJECTIVE 'accuracy' \n",
    "SETTINGS (S3_BUCKET '<< replace S3 output bucket >>', MAX_RUNTIME 900);\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show model for Iris data set for Use case 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "/* -- show pre built model for iris -- */\n",
    " SHOW model model_iris;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql \n",
    "/* -- show model for model created during the workshop -- */\n",
    " SHOW model model_iris_v2;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check Inference/Accuracy of the model `model_iris` .\n",
    "This is where you run the query to check the accuracy of the models. We will use the function created by the pre built model for the inference and against the data set in inference table `iris_data_test`. \n",
    "\n",
    "Please feel free to run the same against the function that was created by the model we created in the workshop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "/* -- Inference query for iris data set -- */\n",
    "WITH infer_data AS (\n",
    "    SELECT Species AS label,\n",
    "        func_model_iris(Id, SepalLengthCm, SepalWidthCm, PetalLengthCm, PetalWidthCm) AS predicted,\n",
    "        CASE WHEN label is NULL THEN NULL ELSE label END AS actual,\n",
    "        CASE WHEN actual = predicted THEN 1::INT\n",
    "        ELSE 0::INT END AS correct\n",
    "    FROM iris_data_test\n",
    "),\n",
    "aggr_data AS (\n",
    "    SELECT SUM(correct) as num_correct, COUNT(*) as total FROM infer_data\n",
    ")\n",
    "SELECT (num_correct::float/total::float) AS accuracy FROM aggr_data;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predict the class of the Iris flower using the testing data set \n",
    "We are running this query against the dataset in inference table `iris_data_set`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sample output for prediction \n",
    "\n",
    "```sql\n",
    "dev-# from class_data GROUP BY 1;\n",
    "  class_distribution   | count\n",
    "-----------------------+-------\n",
    " Class-Iris-versicolor |    82\n",
    " Class-Iris-setosa     |    81\n",
    " Class-Iris-virginica  |    88\n",
    "(3 rows)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "/* -- Predict the Iris flower class -- */ \n",
    "WITH class_data AS ( SELECT func_model_iris( \n",
    "   Id,\n",
    "   SepalLengthCm,\n",
    "   SepalWidthCm,\n",
    "   PetalLengthCm,\n",
    "   PetalWidthCm) AS class \n",
    "FROM iris_data_test )\n",
    "SELECT \n",
    "CASE WHEN class = 'Iris-versicolor'  THEN 'Class-Iris-versicolor'\n",
    "     WHEN class = 'Iris-setosa'  THEN 'Class-Iris-setosa'\n",
    "     WHEN class = 'Iris-virginica'  THEN 'Class-Iris-virginica'\n",
    "     ELSE 'Class-Other' END as class_distribution,\n",
    "COUNT(1) AS count\n",
    "from class_data GROUP BY 1;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Redshift-ML-Workshop - Usecase 3 - Data Scientist / Machine Learning Expert \n",
    "---\n",
    "\n",
    "### Data Set Information: ###\n",
    "\n",
    "Predicting the age of abalone from physical measurements. The age of abalone is determined by cutting the shell through the cone, staining it, and counting the number of rings through a microscope -- a boring and time-consuming task. Other measurements, which are easier to obtain, are used to predict the age. Further information, such as weather patterns and location (hence food availability) may be required to solve the problem.\n",
    "\n",
    "From the original data examples with missing values were removed (the majority having the predicted value missing), and the ranges of the continuous values have been scaled for use with an ANN (by dividing by 200).\n",
    "\n",
    "\n",
    "Attribute Information:\n",
    "\n",
    "Given is the attribute name, attribute type, the measurement unit and a brief description. The number of rings is the value to predict: either as a continuous value or as a classification problem.\n",
    "\n",
    "### Name / Data Type / Measurement Unit / Description ###\n",
    "---\n",
    "Sex / nominal / -- / M, F, and I (infant)  \n",
    "Length / continuous / mm / Longest shell measurement  \n",
    "Diameter / continuous / mm / perpendicular to length  \n",
    "Height / continuous / mm / with meat in shell  \n",
    "Whole weight / continuous / grams / whole abalone \n",
    "Shucked weight / continuous / grams / weight of meat  \n",
    "Viscera weight / continuous / grams / gut weight (after bleeding)  \n",
    "Shell weight / continuous / grams / after being dried  \n",
    "Rings / integer / -- / +1.5 gives the age in years  \n",
    "  \n",
    "*Reference* : https://archive.ics.uci.edu/ml/datasets/Abalone  \n",
    "\n",
    "For this example, the user is considered advanced machine learning expert where the autopilot is not used and the user will directly provide advanced properties including `preprocessors` and `hyper parameters` . \n",
    "\n",
    "\n",
    "For this example, we are going to provide the `MODEL_TYPE` , `OBJECTIVE`, `PREPROCESSORS` and `HYPER PARAMETERS`. \n",
    "\n",
    "For all options supported - https://docs.aws.amazon.com/redshift/latest/dg/r_CREATE_MODEL.html#r_auto_off_create_model\n",
    "\n",
    "\n",
    "Complete SQL is here <insert sql here > \n",
    "\n",
    "Sample dataset is already loaded into `abalone_xgb_train` and `abalone_xgb_test` tables.\n",
    "\n",
    "We will run the create model live in the session using `xgboost` which should take ~15 mins. \n",
    "\n",
    "For the inference queries we can use SQL function created by the model .\n",
    "\n",
    "Before running the create model we will also create the tables and load sample data into it. Make sure to change the `IAM role` in the COPY statement. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "/* -- Create table for xgboost model training -- */\n",
    "CREATE TABLE abalone_xgb_train (\n",
    "length_val float, \n",
    "diameter float, \n",
    "height float,\n",
    "whole_weight float, \n",
    "shucked_weight float, \n",
    "viscera_weight float,\n",
    "shell_weight float, \n",
    "rings int\n",
    ");\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "/* -- Create table for xgboost model testing -- */\n",
    "CREATE TABLE abalone_xgb_test (\n",
    "length_val float, \n",
    "diameter float, \n",
    "height float,\n",
    "whole_weight float, \n",
    "shucked_weight float, \n",
    "viscera_weight float,\n",
    "shell_weight float, \n",
    "rings int\n",
    ");\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "/* -- COPY for abalone_xgb_train -- */\n",
    "COPY abalone_xgb_train FROM 's3://redshift-downloads/redshift-ml/workshop/xgboost_abalone_data/train/' REGION 'us-east-1' IAM_ROLE '<< replace IAM role arn >>' IGNOREHEADER 1 CSV;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "/* -- COPY for abalone_xgb_test -- */\n",
    "COPY abalone_xgb_test FROM 's3://redshift-downloads/redshift-ml/workshop/xgboost_abalone_data/test/' REGION 'us-east-1' IAM_ROLE '<< replace IAM role arn >>' IGNOREHEADER 1 CSV;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "/* -- Create model -- */\n",
    "CREATE MODEL model_abalone_xgboost_regression \n",
    "FROM (SELECT\n",
    "      length_val,\n",
    "      diameter,\n",
    "      height,\n",
    "      whole_weight,\n",
    "      shucked_weight,\n",
    "      viscera_weight,\n",
    "      shell_weight,\n",
    "      rings\n",
    "     FROM abalone_xgb_train)\n",
    "TARGET Rings \n",
    "FUNCTION func_model_abalone_xgboost_regression \n",
    "IAM_ROLE '<< replace IAM role arn >>' \n",
    "AUTO OFF \n",
    "MODEL_TYPE xgboost \n",
    "OBJECTIVE 'reg:squarederror' \n",
    "PREPROCESSORS 'none' \n",
    "HYPERPARAMETERS DEFAULT EXCEPT (NUM_ROUND '100') \n",
    "SETTINGS (S3_BUCKET '<< S3 bucket >>');\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show model for xgboost ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SHOW model model_abalone_xgboost_regression;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check Inference/accuracy of the model ####\n",
    "MSE/RMSE [The lower the better]: For regression problems, we compute Mean Squared Error / Root Mean Squared Error for accuracy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "/* -- Accuracy query -- */\n",
    "WITH infer_data AS (\n",
    "    SELECT Rings AS label, func_model_abalone_xgboost_regression(\n",
    "Length_val, Diameter, Height, Whole_weight, Shucked_weight, Viscera_weight,\n",
    "Shell_weight\n",
    ") AS predicted,\n",
    "    CASE WHEN label is NULL THEN 0 ELSE label END AS actual\n",
    "    FROM abalone_xgb_test\n",
    ")\n",
    "SELECT SQRT(AVG(POWER(actual - predicted, 2))) AS rmse FROM infer_data;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predict the age group of Abalone Species for harvesting, run on the test table #### \n",
    "\n",
    "Sample output\n",
    "\n",
    "```sql\n",
    "     age_group     | count\n",
    "-------------------+-------\n",
    " age_between_10_20 |   589\n",
    " age_between_5_10  |   247\n",
    " age_5_and_under   |     1\n",
    " age_over_20       |     1\n",
    "(4 rows)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "/* -- Prediction query -- */\n",
    "WITH age_data AS ( SELECT func_model_abalone_xgboost_regression( length_val, \n",
    "                                               diameter, \n",
    "                                               height, \n",
    "                                               whole_weight, \n",
    "                                               shucked_weight, \n",
    "                                               viscera_weight, \n",
    "                                               shell_weight ) + 1.5 AS age\n",
    "FROM abalone_xgb_test )\n",
    "SELECT \n",
    "CASE WHEN age  > 20 THEN 'age_over_20'\n",
    "     WHEN age  > 10 THEN 'age_between_10_20'\n",
    "     WHEN age  > 5  THEN 'age_between_5_10'\n",
    "     ELSE 'age_5_and_under' END as age_group,\n",
    "COUNT(1) AS count\n",
    "from age_data GROUP BY 1;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# System Tables for debugging #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "/* -- stv_ml_model_info -- */\n",
    "SELECT * FROM stv_ml_model_info WHERE model_name='model_abalone_xgboost_regression';\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
