{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import botocore.session as s\n",
    "from botocore.exceptions import ClientError\n",
    "import boto3.session\n",
    "import json\n",
    "import boto3\n",
    "import sagemaker\n",
    "import s3fs\n",
    "import time\n",
    "import os\n",
    "import random\n",
    "import datetime\n",
    "import operator\n",
    "from botocore.exceptions import WaiterError\n",
    "from botocore.waiter import WaiterModel\n",
    "from botocore.waiter import create_waiter_with_client\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import IntSlider, FloatSlider, Checkbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create custom waiter for the Redshift Data API to wait for finish execution of current SQL statement\n",
    "waiter_name = 'DataAPIExecution'\n",
    "\n",
    "delay=2\n",
    "max_attempts=3\n",
    "\n",
    "#Configure the waiter settings\n",
    "waiter_config = {\n",
    "  'version': 2,\n",
    "  'waiters': {\n",
    "    'DataAPIExecution': {\n",
    "      'operation': 'DescribeStatement',\n",
    "      'delay': delay,\n",
    "      'maxAttempts': max_attempts,\n",
    "      'acceptors': [\n",
    "        {\n",
    "          \"matcher\": \"path\",\n",
    "          \"expected\": \"FINISHED\",\n",
    "          \"argument\": \"Status\",\n",
    "          \"state\": \"success\"\n",
    "        },\n",
    "        {\n",
    "          \"matcher\": \"pathAny\",\n",
    "          \"expected\": [\"PICKED\",\"STARTED\",\"SUBMITTED\"],\n",
    "          \"argument\": \"Status\",\n",
    "          \"state\": \"retry\"\n",
    "        },\n",
    "        {\n",
    "          \"matcher\": \"pathAny\",\n",
    "          \"expected\": [\"FAILED\",\"ABORTED\"],\n",
    "          \"argument\": \"Status\",\n",
    "          \"state\": \"failure\"\n",
    "        }\n",
    "      ],\n",
    "    },\n",
    "  },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker_session = sagemaker.Session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before starting, we can override the default values for the following:\n",
    "- The S3 bucket and prefix that you want to use for training and model data. This should be within the same region as the Notebook Instance, training, and hosting.\n",
    "- The IAM role arn used to give training and hosting access to your data. See the documentation for how to create these."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieve DB details from AWS Secrets Manager\n",
    "We need to retrieve from AWS Secrets Manager-\n",
    "* Cluster Identifier\n",
    "* Secrets ARN\n",
    "* Database name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster_id: redshift-cluster-1\n",
      "DB: dev\n",
      "Secret ARN: arn:aws:secretsmanager:us-east-1:xxxxx:secret:SecretRedshiftMasterUser-xxxx-IZpZIl\n"
     ]
    }
   ],
   "source": [
    "secret_name='SecretRedshiftMasterUser-jNyiUN1DweVn' ## replace the secret name with yours\n",
    "session = boto3.session.Session()\n",
    "region = session.region_name\n",
    "\n",
    "client = session.client(\n",
    "        service_name='secretsmanager',\n",
    "        region_name=region\n",
    "    )\n",
    "\n",
    "try:\n",
    "    get_secret_value_response = client.get_secret_value(\n",
    "            SecretId=secret_name\n",
    "        )\n",
    "    secret_arn=get_secret_value_response['ARN']\n",
    "\n",
    "except ClientError as e:\n",
    "    print(\"Error retrieving secret. Error: \" + e.response['Error']['Message'])\n",
    "    \n",
    "else:\n",
    "    # Depending on whether the secret is a string or binary, one of these fields will be populated.\n",
    "    if 'SecretString' in get_secret_value_response:\n",
    "        secret = get_secret_value_response['SecretString']\n",
    "    else:\n",
    "        secret = base64.b64decode(get_secret_value_response['SecretBinary'])\n",
    "            \n",
    "secret_json = json.loads(secret)\n",
    "\n",
    "cluster_id=secret_json['dbClusterIdentifier']\n",
    "db=secret_json['dbname']\n",
    "print(\"Cluster_id: \" + cluster_id + \"\\nDB: \" + db + \"\\nSecret ARN: \" + secret_arn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will create the Data API client. For the rest of the notebook we will use this Data API client `client_redshift`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data API client successfully loaded\n"
     ]
    }
   ],
   "source": [
    "bc_session = s.get_session()\n",
    "\n",
    "session = boto3.Session(\n",
    "        botocore_session=bc_session,\n",
    "        region_name=region,\n",
    "    )\n",
    "\n",
    "# Setup the client\n",
    "client_redshift = session.client(\"redshift-data\")\n",
    "print(\"Data API client successfully loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## List Schema\n",
    "We first list the schema in current database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['catalog_history',\n",
       " 'information_schema',\n",
       " 'marketing',\n",
       " 'public',\n",
       " 'public_raw',\n",
       " 'pg_catalog',\n",
       " 'pg_internal',\n",
       " 'public',\n",
       " 'sales']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client_redshift.list_schemas(\n",
    "    Database= db, \n",
    "    SecretArn= secret_arn, \n",
    "    ClusterIdentifier= cluster_id)[\"Schemas\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'categories', 'schema': 'public', 'type': 'TABLE'},\n",
       " {'name': 'holidays', 'schema': 'public', 'type': 'TABLE'},\n",
       " {'name': 'mv_tbl__sales_quantity_mv__0', 'schema': 'public', 'type': 'TABLE'},\n",
       " {'name': 'product_sales', 'schema': 'public', 'type': 'TABLE'},\n",
       " {'name': 'products', 'schema': 'public', 'type': 'TABLE'},\n",
       " {'name': 'sales', 'schema': 'public', 'type': 'TABLE'},\n",
       " {'name': 'sales_model_data', 'schema': 'public', 'type': 'TABLE'},\n",
       " {'name': 'sales_quantity_mv', 'schema': 'public', 'type': 'VIEW'}]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client_redshift.list_tables(\n",
    "    Database= db, \n",
    "    SecretArn= secret_arn, \n",
    "    SchemaPattern='public',\n",
    "    TablePattern='%',\n",
    "    ClusterIdentifier= cluster_id)[\"Tables\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initiating Waiter\n",
    "Initiating the custom waiter for subseqent `execute_statement` Data API call to return FINISHED signal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "waiter_model = WaiterModel(waiter_config)\n",
    "custom_waiter = create_waiter_with_client(waiter_name, waiter_model, client_redshift)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the schema `taxischema` and the table `nyc_greentaxi`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'categories', 'schema': 'public', 'type': 'TABLE'},\n",
       " {'name': 'holidays', 'schema': 'public', 'type': 'TABLE'},\n",
       " {'name': 'mv_tbl__sales_quantity_mv__0', 'schema': 'public', 'type': 'TABLE'},\n",
       " {'name': 'product_sales', 'schema': 'public', 'type': 'TABLE'},\n",
       " {'name': 'products', 'schema': 'public', 'type': 'TABLE'},\n",
       " {'name': 'sales', 'schema': 'public', 'type': 'TABLE'},\n",
       " {'name': 'sales_model_data', 'schema': 'public', 'type': 'TABLE'},\n",
       " {'name': 'sales_quantity_mv', 'schema': 'public', 'type': 'VIEW'}]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client_redshift.list_tables(\n",
    "    Database= db, \n",
    "    SecretArn= secret_arn, \n",
    "    SchemaPattern='public',\n",
    "    ClusterIdentifier= cluster_id)[\"Tables\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Redshift Data API execution  started ...\n",
      "Done waiting to finish Data API.\n",
      "Status: FINISHED. Excution time: 3 miliseconds\n"
     ]
    }
   ],
   "source": [
    "query_str = 'select sysdate;'\n",
    "\n",
    "res = client_redshift.execute_statement(Database= db, SecretArn= secret_arn, Sql= query_str, ClusterIdentifier= cluster_id)\n",
    "print(\"Redshift Data API execution  started ...\")\n",
    "id=res[\"Id\"]\n",
    "\n",
    "# Waiter in try block and wait for DATA API to return\n",
    "try:\n",
    "    custom_waiter.wait(Id=id)\n",
    "    print(\"Done waiting to finish Data API.\")\n",
    "except WaiterError as e:\n",
    "    print (e)\n",
    "    \n",
    "desc=client_redshift.describe_statement(Id=id)\n",
    "print(\"Status: \" + desc[\"Status\"] + \". Excution time: %d miliseconds\" %float(desc[\"Duration\"]/pow(10,6)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In-place analysis\n",
    "\n",
    "We can execute Redshfit data API to fetch the query result into the Panda data frame. This simplifies the in-place analysis of Amazon Redshift cluster data since this bypasses UNLOAD-ing the data first into Amazon S3 and then loading into the Panda data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Redshift Data API execution  started ...\n",
      "Done waiting to finish Data API.\n",
      "\n",
      "Table rows count: 1705\n"
     ]
    }
   ],
   "source": [
    "query_str = \"select count(*) from public.categories;\"\n",
    "\n",
    "res = client_redshift.execute_statement(Database= db, SecretArn= secret_arn, Sql= query_str, ClusterIdentifier= cluster_id)\n",
    "print(\"Redshift Data API execution  started ...\")\n",
    "id = res[\"Id\"]\n",
    "\n",
    "# Reset the 'delay' attribute of the waiter back to 2 seconds.\n",
    "waiter_config[\"waiters\"][\"DataAPIExecution\"][\"delay\"] = 2\n",
    "waiter_model = WaiterModel(waiter_config)\n",
    "custom_waiter = create_waiter_with_client(waiter_name, waiter_model, client_redshift)\n",
    "\n",
    "# Waiter in try block and wait for DATA API to return\n",
    "try:\n",
    "    custom_waiter.wait(Id=id)\n",
    "    print(\"Done waiting to finish Data API.\")\n",
    "except WaiterError as e:\n",
    "    print (e)\n",
    "\n",
    "output=client_redshift.get_statement_result(Id=id)\n",
    "nrows=output[\"TotalNumRows\"]\n",
    "row_count=output[\"Records\"][0][0]\n",
    "\n",
    "print(\"\\nTable rows count: %d\" %row_count[\"longValue\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done waiting to finish Data API.\n",
      "print rows 9999\n",
      "print cols 27\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qty</th>\n",
       "      <th>store_hk</th>\n",
       "      <th>category_hk</th>\n",
       "      <th>item_hk</th>\n",
       "      <th>item_iscorporate</th>\n",
       "      <th>item_isstocked</th>\n",
       "      <th>item_includeai</th>\n",
       "      <th>item_isdepartmentitem</th>\n",
       "      <th>item_isedi</th>\n",
       "      <th>item_ispriceunitonlabel</th>\n",
       "      <th>...</th>\n",
       "      <th>category_iscosmetic</th>\n",
       "      <th>category_isotc</th>\n",
       "      <th>category_isrx</th>\n",
       "      <th>category_iscoupon</th>\n",
       "      <th>transaction_year</th>\n",
       "      <th>transaction_quarter</th>\n",
       "      <th>transaction_month</th>\n",
       "      <th>transaction_month_week</th>\n",
       "      <th>transaction_week_day</th>\n",
       "      <th>holiday</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>4B74DEFC64686F120696C43AEAFB228B</td>\n",
       "      <td>9A37909EC1DFACE5C7FDD46C66B604DD</td>\n",
       "      <td>5D524384B82482DF6BCA5E0C87C5E822</td>\n",
       "      <td>true</td>\n",
       "      <td>true</td>\n",
       "      <td>true</td>\n",
       "      <td>false</td>\n",
       "      <td>false</td>\n",
       "      <td>false</td>\n",
       "      <td>...</td>\n",
       "      <td>false</td>\n",
       "      <td>false</td>\n",
       "      <td>false</td>\n",
       "      <td>false</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>C11DCF9B2E22B2E6C6A424EFACB2528C</td>\n",
       "      <td>9A37909EC1DFACE5C7FDD46C66B604DD</td>\n",
       "      <td>D031649596FD078F28A87F353A13EDD1</td>\n",
       "      <td>true</td>\n",
       "      <td>true</td>\n",
       "      <td>true</td>\n",
       "      <td>false</td>\n",
       "      <td>false</td>\n",
       "      <td>false</td>\n",
       "      <td>...</td>\n",
       "      <td>false</td>\n",
       "      <td>false</td>\n",
       "      <td>false</td>\n",
       "      <td>false</td>\n",
       "      <td>19</td>\n",
       "      <td>4</td>\n",
       "      <td>12</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>52830DBF56F587767F810C8BC78A26E9</td>\n",
       "      <td>9A37909EC1DFACE5C7FDD46C66B604DD</td>\n",
       "      <td>B83A43C1AC9C8775B8CB2BB7C648F735</td>\n",
       "      <td>true</td>\n",
       "      <td>true</td>\n",
       "      <td>true</td>\n",
       "      <td>false</td>\n",
       "      <td>false</td>\n",
       "      <td>false</td>\n",
       "      <td>...</td>\n",
       "      <td>false</td>\n",
       "      <td>true</td>\n",
       "      <td>false</td>\n",
       "      <td>false</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>E4F6BF766CFF69E86A96B8E97F497E36</td>\n",
       "      <td>9A37909EC1DFACE5C7FDD46C66B604DD</td>\n",
       "      <td>8B4E982774C9418F780574F611EF897B</td>\n",
       "      <td>true</td>\n",
       "      <td>false</td>\n",
       "      <td>false</td>\n",
       "      <td>false</td>\n",
       "      <td>false</td>\n",
       "      <td>false</td>\n",
       "      <td>...</td>\n",
       "      <td>false</td>\n",
       "      <td>false</td>\n",
       "      <td>false</td>\n",
       "      <td>false</td>\n",
       "      <td>20</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>69184604752C65742866D1ABF7F68732</td>\n",
       "      <td>9A37909EC1DFACE5C7FDD46C66B604DD</td>\n",
       "      <td>96A78731F20599B99FF0461A36B1FD09</td>\n",
       "      <td>true</td>\n",
       "      <td>true</td>\n",
       "      <td>true</td>\n",
       "      <td>false</td>\n",
       "      <td>false</td>\n",
       "      <td>false</td>\n",
       "      <td>...</td>\n",
       "      <td>true</td>\n",
       "      <td>false</td>\n",
       "      <td>false</td>\n",
       "      <td>false</td>\n",
       "      <td>20</td>\n",
       "      <td>4</td>\n",
       "      <td>12</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9994</th>\n",
       "      <td>1.0</td>\n",
       "      <td>B68484EA6F64CD6BD5F637A2CA7129F3</td>\n",
       "      <td>9A37909EC1DFACE5C7FDD46C66B604DD</td>\n",
       "      <td>235074D2F4845F432329E55792DF4574</td>\n",
       "      <td>true</td>\n",
       "      <td>true</td>\n",
       "      <td>true</td>\n",
       "      <td>false</td>\n",
       "      <td>false</td>\n",
       "      <td>false</td>\n",
       "      <td>...</td>\n",
       "      <td>false</td>\n",
       "      <td>false</td>\n",
       "      <td>false</td>\n",
       "      <td>false</td>\n",
       "      <td>19</td>\n",
       "      <td>4</td>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>1.0</td>\n",
       "      <td>DE192B3A48262A654E50849BBABA9203</td>\n",
       "      <td>9A37909EC1DFACE5C7FDD46C66B604DD</td>\n",
       "      <td>7A6120954AC417EC990372724482EB6E</td>\n",
       "      <td>true</td>\n",
       "      <td>true</td>\n",
       "      <td>true</td>\n",
       "      <td>false</td>\n",
       "      <td>false</td>\n",
       "      <td>false</td>\n",
       "      <td>...</td>\n",
       "      <td>false</td>\n",
       "      <td>true</td>\n",
       "      <td>false</td>\n",
       "      <td>false</td>\n",
       "      <td>19</td>\n",
       "      <td>4</td>\n",
       "      <td>12</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>1.0</td>\n",
       "      <td>66CEB41858A514D81D2F47167D9CCFCD</td>\n",
       "      <td>9A37909EC1DFACE5C7FDD46C66B604DD</td>\n",
       "      <td>B12D1B51499A5A5FD4048D8A1C4E276F</td>\n",
       "      <td>true</td>\n",
       "      <td>true</td>\n",
       "      <td>true</td>\n",
       "      <td>false</td>\n",
       "      <td>false</td>\n",
       "      <td>false</td>\n",
       "      <td>...</td>\n",
       "      <td>false</td>\n",
       "      <td>false</td>\n",
       "      <td>false</td>\n",
       "      <td>false</td>\n",
       "      <td>20</td>\n",
       "      <td>4</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>1.0</td>\n",
       "      <td>CC7B44B8053EB838F14898987F892010</td>\n",
       "      <td>9A37909EC1DFACE5C7FDD46C66B604DD</td>\n",
       "      <td>71D496C1B813093F1A6A21821CA31174</td>\n",
       "      <td>true</td>\n",
       "      <td>true</td>\n",
       "      <td>true</td>\n",
       "      <td>false</td>\n",
       "      <td>false</td>\n",
       "      <td>false</td>\n",
       "      <td>...</td>\n",
       "      <td>false</td>\n",
       "      <td>false</td>\n",
       "      <td>false</td>\n",
       "      <td>false</td>\n",
       "      <td>21</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>2.0</td>\n",
       "      <td>1659C9BC27D9EEDBA18ABDF6D888E01C</td>\n",
       "      <td>9A37909EC1DFACE5C7FDD46C66B604DD</td>\n",
       "      <td>AEC0EAB815104D9F06D6C2DC6D064B50</td>\n",
       "      <td>true</td>\n",
       "      <td>true</td>\n",
       "      <td>true</td>\n",
       "      <td>false</td>\n",
       "      <td>false</td>\n",
       "      <td>true</td>\n",
       "      <td>...</td>\n",
       "      <td>false</td>\n",
       "      <td>false</td>\n",
       "      <td>false</td>\n",
       "      <td>false</td>\n",
       "      <td>20</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9999 rows × 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      qty                          store_hk                       category_hk  \\\n",
       "0     1.0  4B74DEFC64686F120696C43AEAFB228B  9A37909EC1DFACE5C7FDD46C66B604DD   \n",
       "1     1.0  C11DCF9B2E22B2E6C6A424EFACB2528C  9A37909EC1DFACE5C7FDD46C66B604DD   \n",
       "2     1.0  52830DBF56F587767F810C8BC78A26E9  9A37909EC1DFACE5C7FDD46C66B604DD   \n",
       "3     1.0  E4F6BF766CFF69E86A96B8E97F497E36  9A37909EC1DFACE5C7FDD46C66B604DD   \n",
       "4     1.0  69184604752C65742866D1ABF7F68732  9A37909EC1DFACE5C7FDD46C66B604DD   \n",
       "...   ...                               ...                               ...   \n",
       "9994  1.0  B68484EA6F64CD6BD5F637A2CA7129F3  9A37909EC1DFACE5C7FDD46C66B604DD   \n",
       "9995  1.0  DE192B3A48262A654E50849BBABA9203  9A37909EC1DFACE5C7FDD46C66B604DD   \n",
       "9996  1.0  66CEB41858A514D81D2F47167D9CCFCD  9A37909EC1DFACE5C7FDD46C66B604DD   \n",
       "9997  1.0  CC7B44B8053EB838F14898987F892010  9A37909EC1DFACE5C7FDD46C66B604DD   \n",
       "9998  2.0  1659C9BC27D9EEDBA18ABDF6D888E01C  9A37909EC1DFACE5C7FDD46C66B604DD   \n",
       "\n",
       "                               item_hk item_iscorporate item_isstocked  \\\n",
       "0     5D524384B82482DF6BCA5E0C87C5E822             true           true   \n",
       "1     D031649596FD078F28A87F353A13EDD1             true           true   \n",
       "2     B83A43C1AC9C8775B8CB2BB7C648F735             true           true   \n",
       "3     8B4E982774C9418F780574F611EF897B             true          false   \n",
       "4     96A78731F20599B99FF0461A36B1FD09             true           true   \n",
       "...                                ...              ...            ...   \n",
       "9994  235074D2F4845F432329E55792DF4574             true           true   \n",
       "9995  7A6120954AC417EC990372724482EB6E             true           true   \n",
       "9996  B12D1B51499A5A5FD4048D8A1C4E276F             true           true   \n",
       "9997  71D496C1B813093F1A6A21821CA31174             true           true   \n",
       "9998  AEC0EAB815104D9F06D6C2DC6D064B50             true           true   \n",
       "\n",
       "     item_includeai item_isdepartmentitem item_isedi item_ispriceunitonlabel  \\\n",
       "0              true                 false      false                   false   \n",
       "1              true                 false      false                   false   \n",
       "2              true                 false      false                   false   \n",
       "3             false                 false      false                   false   \n",
       "4              true                 false      false                   false   \n",
       "...             ...                   ...        ...                     ...   \n",
       "9994           true                 false      false                   false   \n",
       "9995           true                 false      false                   false   \n",
       "9996           true                 false      false                   false   \n",
       "9997           true                 false      false                   false   \n",
       "9998           true                 false      false                    true   \n",
       "\n",
       "      ... category_iscosmetic category_isotc category_isrx category_iscoupon  \\\n",
       "0     ...               false          false         false             false   \n",
       "1     ...               false          false         false             false   \n",
       "2     ...               false           true         false             false   \n",
       "3     ...               false          false         false             false   \n",
       "4     ...                true          false         false             false   \n",
       "...   ...                 ...            ...           ...               ...   \n",
       "9994  ...               false          false         false             false   \n",
       "9995  ...               false           true         false             false   \n",
       "9996  ...               false          false         false             false   \n",
       "9997  ...               false          false         false             false   \n",
       "9998  ...               false          false         false             false   \n",
       "\n",
       "     transaction_year transaction_quarter transaction_month  \\\n",
       "0                  20                   1                 3   \n",
       "1                  19                   4                12   \n",
       "2                  20                   1                 3   \n",
       "3                  20                   3                 9   \n",
       "4                  20                   4                12   \n",
       "...               ...                 ...               ...   \n",
       "9994               19                   4                12   \n",
       "9995               19                   4                12   \n",
       "9996               20                   4                12   \n",
       "9997               21                   1                 1   \n",
       "9998               20                   2                 5   \n",
       "\n",
       "     transaction_month_week transaction_week_day holiday  \n",
       "0                         1                    1       0  \n",
       "1                         4                    7       0  \n",
       "2                         4                    7       0  \n",
       "3                         2                    7       0  \n",
       "4                         5                    4       1  \n",
       "...                     ...                  ...     ...  \n",
       "9994                      2                    3       0  \n",
       "9995                      5                    2       1  \n",
       "9996                      1                    6       0  \n",
       "9997                      2                    6       0  \n",
       "9998                      5                    7       0  \n",
       "\n",
       "[9999 rows x 27 columns]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_str = \"\"\"SELECT abs(quantity) as qty,store_hk,\n",
    "       category_hk,\n",
    "       item_hk,\n",
    "       item_iscorporate,\n",
    "       item_isstocked,\n",
    "       item_includeai,\n",
    "       item_isdepartmentitem,\n",
    "       item_isedi,\n",
    "       item_ispriceunitonlabel,\n",
    "       item_issendtomorgue,\n",
    "       item_isinactive,\n",
    "       item_isconverted,\n",
    "       item_isdiscontinued,\n",
    "       item_isincludeexo,\n",
    "       item_isbasketactive,\n",
    "       item_islocalitemreturnloss,\n",
    "       category_iscosmetic,\n",
    "       category_isotc,\n",
    "       category_isrx,\n",
    "       category_iscoupon,\n",
    "       transaction_year,\n",
    "       transaction_quarter,\n",
    "       transaction_month,\n",
    "       transaction_month_week,\n",
    "       transaction_week_day,\n",
    "       holiday\n",
    "FROM public.product_sales WHERE sl <10000\"\"\"\n",
    "\n",
    "res = client_redshift.execute_statement(Database= db, SecretArn= secret_arn, Sql= query_str, ClusterIdentifier= cluster_id)\n",
    "\n",
    "id = res[\"Id\"]\n",
    "# Waiter in try block and wait for DATA API to return\n",
    "try:\n",
    "    custom_waiter.wait(Id=id)\n",
    "    print(\"Done waiting to finish Data API.\")\n",
    "except WaiterError as e:\n",
    "    print (e)\n",
    "\n",
    "output=client_redshift.get_statement_result(Id=id)\n",
    "nrows=output[\"TotalNumRows\"]\n",
    "ncols=len(output[\"ColumnMetadata\"])\n",
    "print(\"print rows \" + str(nrows))\n",
    "print(\"print cols \" + str(ncols))\n",
    "\n",
    "\n",
    "\n",
    "#print(\"Number of columns: %d\" %ncols)\n",
    "resultrows=output[\"Records\"]\n",
    "\n",
    "col_labels=[]\n",
    "for i in range(ncols): col_labels.append(output[\"ColumnMetadata\"][i]['label'])\n",
    "                                              \n",
    "records=[]\n",
    "for i in range(nrows): records.append(resultrows[i])\n",
    "\n",
    "df = pd.DataFrame(np.array(resultrows), columns=col_labels)\n",
    "\n",
    "df[col_labels[0]]=df[col_labels[0]].apply(operator.itemgetter('doubleValue'))\n",
    "df[col_labels[1]]=df[col_labels[1]].apply(operator.itemgetter('stringValue'))\n",
    "df[col_labels[2]]=df[col_labels[2]].apply(operator.itemgetter('stringValue'))\n",
    "df[col_labels[3]]=df[col_labels[3]].apply(operator.itemgetter('stringValue'))\n",
    "df[col_labels[4]]=df[col_labels[4]].apply(operator.itemgetter('stringValue'))\n",
    "df[col_labels[5]]=df[col_labels[5]].apply(operator.itemgetter('stringValue'))\n",
    "df[col_labels[6]]=df[col_labels[6]].apply(operator.itemgetter('stringValue'))\n",
    "df[col_labels[7]]=df[col_labels[7]].apply(operator.itemgetter('stringValue'))\n",
    "df[col_labels[8]]=df[col_labels[8]].apply(operator.itemgetter('stringValue'))\n",
    "df[col_labels[9]]=df[col_labels[9]].apply(operator.itemgetter('stringValue'))\n",
    "df[col_labels[10]]=df[col_labels[10]].apply(operator.itemgetter('stringValue'))\n",
    "df[col_labels[11]]=df[col_labels[11]].apply(operator.itemgetter('stringValue'))\n",
    "df[col_labels[12]]=df[col_labels[12]].apply(operator.itemgetter('stringValue'))\n",
    "df[col_labels[13]]=df[col_labels[13]].apply(operator.itemgetter('stringValue'))\n",
    "df[col_labels[14]]=df[col_labels[14]].apply(operator.itemgetter('stringValue'))\n",
    "df[col_labels[15]]=df[col_labels[15]].apply(operator.itemgetter('stringValue'))\n",
    "df[col_labels[16]]=df[col_labels[16]].apply(operator.itemgetter('stringValue'))\n",
    "df[col_labels[17]]=df[col_labels[17]].apply(operator.itemgetter('stringValue'))\n",
    "df[col_labels[18]]=df[col_labels[18]].apply(operator.itemgetter('stringValue'))\n",
    "df[col_labels[19]]=df[col_labels[19]].apply(operator.itemgetter('stringValue'))\n",
    "df[col_labels[20]]=df[col_labels[20]].apply(operator.itemgetter('stringValue'))\n",
    "df[col_labels[21]]=df[col_labels[21]].apply(operator.itemgetter('longValue'))\n",
    "df[col_labels[22]]=df[col_labels[22]].apply(operator.itemgetter('longValue'))\n",
    "df[col_labels[23]]=df[col_labels[23]].apply(operator.itemgetter('longValue'))\n",
    "df[col_labels[24]]=df[col_labels[24]].apply(operator.itemgetter('longValue'))\n",
    "df[col_labels[25]]=df[col_labels[25]].apply(operator.itemgetter('longValue'))\n",
    "df[col_labels[26]]=df[col_labels[26]].apply(operator.itemgetter('longValue'))\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ML Model Build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_bucket = sagemaker.Session().default_bucket()  # replace with an existing bucket if needed\n",
    "s3_prefix = 'redshift-deepar-nyctaxi-demo-notebook'    # prefix used for all data stored within the bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://sagemaker-us-east-1-001876746742/redshift-deepar-nyctaxi-demo-notebook/output\n"
     ]
    }
   ],
   "source": [
    "s3_data_path = \"s3://{}/{}/data\".format(s3_bucket, s3_prefix)\n",
    "s3_output_path = \"s3://{}/{}/output\".format(s3_bucket, s3_prefix)\n",
    "print(s3_output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we configure the container image to be used for the region that we are running in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The method get_image_uri has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "Defaulting to the only supported framework/algorithm version: 1. Ignoring framework/algorithm version: latest.\n"
     ]
    }
   ],
   "source": [
    "image_name = sagemaker.amazon.amazon_estimator.get_image_uri(region, \"forecasting-deepar\", \"latest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'redshift_iam_role' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-c6acf844e6a0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mredshift_unload_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ms3_output_path\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'/unload/'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mquery_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"unload('select coalesce(v1.pickup_timestamp_norm, v2.pickup_timestamp_norm) as pickup_timestamp_norm , coalesce(v1.vendor_1, 0) as vendor_1 , coalesce(v2.vendor_2, 0) as vendor_2 from (select case when extract(minute from lpep_dropoff_datetime) between 0 and 14 then dateadd(minute, 0, date_trunc(''hour'', lpep_dropoff_datetime)) when extract(minute from lpep_dropoff_datetime) between 15 and 29 then dateadd(minute, 15, date_trunc(''hour'', lpep_dropoff_datetime)) when extract(minute from lpep_dropoff_datetime) between 30 and 44 then dateadd(minute, 30, date_trunc(''hour'', lpep_dropoff_datetime)) when extract(minute from lpep_dropoff_datetime) between 45 and 59 then dateadd(minute, 45, date_trunc(''hour'', lpep_dropoff_datetime)) end as pickup_timestamp_norm , count(1) as vendor_1 from taxischema.nyc_greentaxi where vendorid = 1 group by 1) v1 full outer join (select case when extract(minute from lpep_dropoff_datetime) between 0 and 14 then dateadd(minute, 0, date_trunc(''hour'', lpep_dropoff_datetime)) when extract(minute from lpep_dropoff_datetime) between 15 and 29 then dateadd(minute, 15, date_trunc(''hour'', lpep_dropoff_datetime)) when extract(minute from lpep_dropoff_datetime) between 30 and 44 then dateadd(minute, 30, date_trunc(''hour'', lpep_dropoff_datetime)) when extract(minute from lpep_dropoff_datetime) between 45 and 59 then dateadd(minute, 45, date_trunc(''hour'', lpep_dropoff_datetime)) end as pickup_timestamp_norm , count(1)  as vendor_2 from taxischema.nyc_greentaxi where vendorid = 2 group by 1) v2 on v1.pickup_timestamp_norm = v2.pickup_timestamp_norm order by pickup_timestamp_norm ;') to '\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mredshift_unload_path\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"' iam_role '\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mredshift_iam_role\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"' format as CSV header ALLOWOVERWRITE GZIP\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclient_redshift\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute_statement\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDatabase\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mdb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSecretArn\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0msecret_arn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSql\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mquery_str\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mClusterIdentifier\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mcluster_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'redshift_iam_role' is not defined"
     ]
    }
   ],
   "source": [
    "redshift_unload_path = s3_output_path + '/unload/'\n",
    "\n",
    "query_str = \"unload('select coalesce(v1.pickup_timestamp_norm, v2.pickup_timestamp_norm) as pickup_timestamp_norm , coalesce(v1.vendor_1, 0) as vendor_1 , coalesce(v2.vendor_2, 0) as vendor_2 from (select case when extract(minute from lpep_dropoff_datetime) between 0 and 14 then dateadd(minute, 0, date_trunc(''hour'', lpep_dropoff_datetime)) when extract(minute from lpep_dropoff_datetime) between 15 and 29 then dateadd(minute, 15, date_trunc(''hour'', lpep_dropoff_datetime)) when extract(minute from lpep_dropoff_datetime) between 30 and 44 then dateadd(minute, 30, date_trunc(''hour'', lpep_dropoff_datetime)) when extract(minute from lpep_dropoff_datetime) between 45 and 59 then dateadd(minute, 45, date_trunc(''hour'', lpep_dropoff_datetime)) end as pickup_timestamp_norm , count(1) as vendor_1 from taxischema.nyc_greentaxi where vendorid = 1 group by 1) v1 full outer join (select case when extract(minute from lpep_dropoff_datetime) between 0 and 14 then dateadd(minute, 0, date_trunc(''hour'', lpep_dropoff_datetime)) when extract(minute from lpep_dropoff_datetime) between 15 and 29 then dateadd(minute, 15, date_trunc(''hour'', lpep_dropoff_datetime)) when extract(minute from lpep_dropoff_datetime) between 30 and 44 then dateadd(minute, 30, date_trunc(''hour'', lpep_dropoff_datetime)) when extract(minute from lpep_dropoff_datetime) between 45 and 59 then dateadd(minute, 45, date_trunc(''hour'', lpep_dropoff_datetime)) end as pickup_timestamp_norm , count(1)  as vendor_2 from taxischema.nyc_greentaxi where vendorid = 2 group by 1) v2 on v1.pickup_timestamp_norm = v2.pickup_timestamp_norm order by pickup_timestamp_norm ;') to '\" + redshift_unload_path + \"' iam_role '\" + redshift_iam_role + \"' format as CSV header ALLOWOVERWRITE GZIP\"\n",
    "\n",
    "res = client_redshift.execute_statement(Database= db, SecretArn= secret_arn, Sql= query_str, ClusterIdentifier= cluster_id)\n",
    "print(\"Redshift Data API execution  started ...\")\n",
    "id = res[\"Id\"]\n",
    "\n",
    "# Reset the 'delay' attribute of the waiter to 20 seconds for the UNLOAD to finish.\n",
    "waiter_config[\"waiters\"][\"DataAPIExecution\"][\"delay\"] = 20\n",
    "waiter_model = WaiterModel(waiter_config)\n",
    "custom_waiter = create_waiter_with_client(waiter_name, waiter_model, client_redshift)\n",
    "\n",
    "# Waiter in try block and wait for DATA API to return\n",
    "try:\n",
    "    custom_waiter.wait(Id=id)\n",
    "    print(\"Done waiting to finish Data API.\")\n",
    "except WaiterError as e:\n",
    "    print (e)\n",
    "    \n",
    "print(\"Query execution complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = boto3.client('s3')\n",
    "\n",
    "def load_df_from_s3(s3_path):\n",
    "    assert s3_path.startswith('s3://')\n",
    "    split = s3_path.split('/')\n",
    "    bucket = split[2]\n",
    "    prefix = '/'.join(split[3:])\n",
    "    print(\"S3 filepath is %s\" %s3_path)\n",
    "\n",
    "  \n",
    "    datafiles = s3.list_objects_v2(Bucket=bucket, Prefix = prefix)['Contents']\n",
    "    prefix_df = []\n",
    "    fs = s3fs.S3FileSystem()\n",
    "\n",
    "    for file in datafiles[0:]:\n",
    "        key = file['Key']\n",
    "\n",
    "        with fs.open('s3://'+ bucket + '/' + key) as f:\n",
    "            df = pd.read_csv(f, compression='gzip', index_col=0, parse_dates=True, decimal=',', sep=',')\n",
    "            #print(\"S3 file %s is reading\" %f)\n",
    "            \n",
    "        prefix_df.append(df)\n",
    "        #print(\"File retrieved %s\" %key)\n",
    "        \n",
    "    return pd.concat(prefix_df)\n",
    "\n",
    "pd_df = load_df_from_s3(redshift_unload_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_timeseries = pd_df.shape[1]\n",
    "data_trip = pd_df.resample('2H').sum() / 8\n",
    "timeseries = []\n",
    "for i in range(num_timeseries):\n",
    "    timeseries.append(np.trim_zeros(data_trip.iloc[:,i], trim='f'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2, figsize=(15, 2), sharex=True)\n",
    "axx = axs.ravel()\n",
    "for i in range(0, 2):\n",
    "    timeseries[i].loc[\"2018-10-01\":\"2020-12-31\"].plot(ax=axx[i])\n",
    "    axx[i].set_xlabel(\"Date\")    \n",
    "    axx[i].set_ylabel(\"Ride count: vendor- %d\" %i)   \n",
    "    axx[i].grid(which='minor', axis='x')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2, figsize=(15, 4), sharex=True)\n",
    "axx = axs.ravel()\n",
    "for i in range(0, 2):\n",
    "    timeseries[i].loc[\"2019-01-01\":\"2019-12-31\"].plot(ax=axx[i])\n",
    "    axx[i].set_xlabel(\"date\")    \n",
    "    axx[i].set_ylabel(\"Ride count: vendor- %d\" %i)   \n",
    "    axx[i].grid(which='minor', axis='x')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and Test splits\n",
    "\n",
    "Often times one is interested in evaluating the model or tuning its hyperparameters by looking at error metrics on a hold-out test set. Here we split the available data into train and test sets for evaluating the trained model. For standard machine learning tasks such as classification and regression, one typically obtains this split by randomly separating examples into train and test sets. However, in forecasting it is important to do this train/test split based on time rather than by time series.\n",
    "\n",
    "In this example, we will reserve the last section of each of the time series for evalutation purpose and use only the first part as training data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we use 2 hour frequency for the time series\n",
    "freq = '2H'\n",
    "\n",
    "# we predict for 7 days\n",
    "prediction_length = 7 * 12\n",
    "\n",
    "# we also use 7 days as context length, this is the number of state updates accomplished before making predictions\n",
    "context_length = 7 * 12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We specify here the portion of the data that is used for training: the model sees data from 2019-01-01 to 2019-04-01 for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_dataset = pd.Timestamp(\"2019-01-01 00:00:00\", freq=freq)\n",
    "end_training = pd.Timestamp(\"2020-01-31 00:00:00\", freq=freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The DeepAR JSON input format represents each time series as a JSON object. In the simplest case each time series just consists of a start time stamp (``start``) and a list of values (``target``). For more complex cases, DeepAR also supports the fields ``dynamic_feat`` for time-series features and ``cat`` for categorical features, which we will use  later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = [\n",
    "    {\n",
    "        \"start\": str(start_dataset),\n",
    "        \"target\": ts[start_dataset:end_training][:-1].tolist()  # We use -1, because pandas indexing includes the upper bound \n",
    "    }\n",
    "    for ts in timeseries\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As test data, we will consider time series extending beyond the training range: these will be used for computing test scores, by using the trained model to forecast their trailing 7 days, and comparing predictions with actual values.\n",
    "To evaluate our model performance on more than one week, we generate test data that extends to 1, 2, 3, 4 weeks beyond the training range. This way we perform *rolling evaluation* of our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_test_windows = 4\n",
    "\n",
    "idx =  []\n",
    "print(len(pd.date_range(start_dataset, end_training )))\n",
    "period_range = len(pd.date_range(start_dataset, end_training))\n",
    "      \n",
    "for i in range(1, num_test_windows + 1) :\n",
    "    idx.append(pd.date_range(start_dataset, periods = period_range + i * prediction_length, freq=freq))\n",
    "\n",
    "test_data = [\n",
    "    {   \n",
    "        \"start\": str(start_dataset),\n",
    "        \"target\": ts[idx[k]].tolist()\n",
    "    }\n",
    "    for k in range(0, num_test_windows)\n",
    "    for ts in timeseries\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now write the dictionary to the `jsonlines` file format that DeepAR understands (it also supports gzipped jsonlines and parquet)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_dicts_to_file(path, data):\n",
    "    with open(path, 'wb') as fp:\n",
    "        for d in data:\n",
    "            fp.write(json.dumps(d).encode(\"utf-8\"))\n",
    "            fp.write(\"\\n\".encode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "write_dicts_to_file(\"train.json\", training_data)\n",
    "write_dicts_to_file(\"test.json\", test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the data files locally, let us copy them to S3 where DeepAR can access them. Depending on your connection, this may take a couple of minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = boto3.resource('s3')\n",
    "def copy_to_s3(local_file, s3_path, override=False):\n",
    "    assert s3_path.startswith('s3://')\n",
    "    split = s3_path.split('/')\n",
    "    bucket = split[2]\n",
    "    path = '/'.join(split[3:])\n",
    "    buk = s3.Bucket(bucket)\n",
    "    \n",
    "    if len(list(buk.objects.filter(Prefix=path))) > 0:\n",
    "        if not override:\n",
    "            print('File s3://{}/{} already exists.\\nSet override to upload anyway.\\n'.format(s3_bucket, s3_path))\n",
    "            return\n",
    "        else:\n",
    "            print('Overwriting existing file')\n",
    "    with open(local_file, 'rb') as data:\n",
    "        print('Uploading file to {}'.format(s3_path))\n",
    "        buk.put_object(Key=path, Body=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "copy_to_s3(\"train.json\", s3_data_path + \"/train/train.json\")\n",
    "copy_to_s3(\"test.json\", s3_data_path + \"/test/test.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look to what we just wrote to S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3filesystem = s3fs.S3FileSystem()\n",
    "with s3filesystem.open(s3_data_path + \"/train/train.json\", 'rb') as fp:\n",
    "    print(fp.readline().decode(\"utf-8\")[:100] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are all set with our dataset processing, we can now call DeepAR to train a model and generate predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a model\n",
    "\n",
    "Here we define the estimator that will launch the training job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = sagemaker.estimator.Estimator(\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    image_name=image_name,\n",
    "    role=redshift_iam_role,\n",
    "    train_instance_count=1,\n",
    "    train_instance_type='ml.c4.2xlarge',\n",
    "    base_job_name='redshift-deepar-nyctaxi-demo',\n",
    "    output_path=s3_output_path\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we need to set the hyperparameters for the training job. For example frequency of the time series used, number of data points the model will look at in the past, number of predicted data points. The other hyperparameters concern the model to train (number of layers, number of cells per layer, likelihood function) and the training options (number of epochs, batch size, learning rate...). We use default parameters for every optional parameter in this case (you can always use Sagemaker Automated Model Tuning to tune them)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {\n",
    "    \"time_freq\": freq,\n",
    "    \"epochs\": \"400\",\n",
    "    \"early_stopping_patience\": \"40\",\n",
    "    \"mini_batch_size\": \"64\",\n",
    "    \"learning_rate\": \"5E-4\",\n",
    "    \"context_length\": str(context_length),\n",
    "    \"prediction_length\": str(prediction_length)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator.set_hyperparameters(**hyperparameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are ready to launch the training job. SageMaker will start an EC2 instance, download the data from S3, start training the model and save the trained model.\n",
    "\n",
    "If you provide the test data channel as we do in this example, DeepAR will also calculate accuracy metrics for the trained model on this test. This is done by predicting the last prediction_length points of each time-series in the test set and comparing this to the actual value of the time-series.\n",
    "\n",
    "Note: the next cell may take a few minutes to complete, depending on data size, model complexity, training options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "data_channels = {\n",
    "    \"train\": \"{}/train/\".format(s3_data_path),\n",
    "    \"test\": \"{}/test/\".format(s3_data_path)\n",
    "}\n",
    "\n",
    "estimator.fit(inputs=data_channels, wait=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since you pass a test set in this example, accuracy metrics for the forecast are computed and logged (see bottom of the log). You can find the definition of these metrics from our documentation. You can use these to optimize the parameters and tune your model or use SageMaker's Automated Model Tuning service to tune the model for you."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   ### Create endpoint and predictor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a trained model, we can use it to perform predictions by deploying it to an endpoint.\n",
    "\n",
    "**Note: Remember to delete the endpoint after running this experiment. A cell at the very bottom of this notebook will do that: make sure you run it at the end.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To query the endpoint and perform predictions, we can define the following utility class: this allows making requests using `pandas.Series` objects rather than raw JSON strings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create endpoint and predictor\n",
    "Now that we have a trained model, we can use it to perform predictions by deploying it to an endpoint.\n",
    "\n",
    "Note: Remember to delete the endpoint after running this experiment. A cell at the very bottom of this notebook will do that: make sure you run it at the end.\n",
    "\n",
    "To query the endpoint and perform predictions, we can define the following utility class: this allows making requests using pandas.Series objects rather than raw JSON strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepARPredictor(sagemaker.predictor.RealTimePredictor):\n",
    "    \n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, content_type=sagemaker.content_types.CONTENT_TYPE_JSON, **kwargs)\n",
    "        \n",
    "    def predict(self, ts, cat=None, dynamic_feat=None, \n",
    "                num_samples=100, return_samples=False, quantiles=[\"0.1\", \"0.5\", \"0.9\"]):\n",
    "        \"\"\"Requests the prediction of for the time series listed in `ts`, each with the (optional)\n",
    "        corresponding category listed in `cat`.\n",
    "        \n",
    "        ts -- `pandas.Series` object, the time series to predict\n",
    "        cat -- integer, the group associated to the time series (default: None)\n",
    "        num_samples -- integer, number of samples to compute at prediction time (default: 100)\n",
    "        return_samples -- boolean indicating whether to include samples in the response (default: False)\n",
    "        quantiles -- list of strings specifying the quantiles to compute (default: [\"0.1\", \"0.5\", \"0.9\"])\n",
    "        \n",
    "        Return value: list of `pandas.DataFrame` objects, each containing the predictions\n",
    "        \"\"\"\n",
    "        prediction_time = ts.index[-1] + pd.Timedelta(freq)\n",
    "        quantiles = [str(q) for q in quantiles]\n",
    "        req = self.__encode_request(ts, cat, dynamic_feat, num_samples, return_samples, quantiles)\n",
    "        res = super(DeepARPredictor, self).predict(req)\n",
    "        return self.__decode_response(res, ts.index.freq, prediction_time, return_samples)\n",
    "    \n",
    "    def __encode_request(self, ts, cat, dynamic_feat, num_samples, return_samples, quantiles):\n",
    "        instance = series_to_dict(ts, cat if cat is not None else None, dynamic_feat if dynamic_feat else None)\n",
    "\n",
    "        configuration = {\n",
    "            \"num_samples\": num_samples,\n",
    "            \"output_types\": [\"quantiles\", \"samples\"] if return_samples else [\"quantiles\"],\n",
    "            \"quantiles\": quantiles\n",
    "        }\n",
    "        \n",
    "        http_request_data = {\n",
    "            \"instances\": [instance],\n",
    "            \"configuration\": configuration\n",
    "        }\n",
    "        \n",
    "        return json.dumps(http_request_data).encode('utf-8')\n",
    "    \n",
    "    def __decode_response(self, response, freq, prediction_time, return_samples):\n",
    "        # we only sent one time series so we only receive one in return\n",
    "        # however, if possible one will pass multiple time series as predictions will then be faster\n",
    "        predictions = json.loads(response.decode('utf-8'))['predictions'][0]\n",
    "        prediction_length = len(next(iter(predictions['quantiles'].values())))\n",
    "        #prediction_index = pd.DatetimeIndex(start=prediction_time, freq=freq, periods=prediction_length)\n",
    "        prediction_index = pd.date_range(prediction_time, periods = prediction_length, freq=freq)\n",
    "        if return_samples:\n",
    "            dict_of_samples = {'sample_' + str(i): s for i, s in enumerate(predictions['samples'])}\n",
    "        else:\n",
    "            dict_of_samples = {}\n",
    "        return pd.DataFrame(data={**predictions['quantiles'], **dict_of_samples}, index=prediction_index)\n",
    "\n",
    "    def set_frequency(self, freq):\n",
    "        self.freq = freq\n",
    "        \n",
    "def encode_target(ts):\n",
    "    return [x if np.isfinite(x) else \"NaN\" for x in ts]        \n",
    "\n",
    "def series_to_dict(ts, cat=None, dynamic_feat=None):\n",
    "    \"\"\"Given a pandas.Series object, returns a dictionary encoding the time series.\n",
    "\n",
    "    ts -- a pands.Series object with the target time series\n",
    "    cat -- an integer indicating the time series category\n",
    "\n",
    "    Return value: a dictionary\n",
    "    \"\"\"\n",
    "    obj = {\"start\": str(ts.index[0]), \"target\": encode_target(ts)}\n",
    "    if cat is not None:\n",
    "        obj[\"cat\"] = cat\n",
    "    if dynamic_feat is not None:\n",
    "        obj[\"dynamic_feat\"] = dynamic_feat        \n",
    "    return obj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can deploy the model and create and endpoint that can be queried using our custom DeepARPredictor class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = estimator.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type='ml.m4.xlarge',\n",
    "    predictor_cls=DeepARPredictor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make predictions and plot results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use the `predictor` object to generate predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.predict(ts=timeseries[1], quantiles=[0.10, 0.5, 0.90]).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we define a plotting function that queries the model and displays the forecast."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(\n",
    "    predictor, \n",
    "    target_ts, \n",
    "    cat=None, \n",
    "    dynamic_feat=None, \n",
    "    forecast_date=end_training, \n",
    "    show_samples=False, \n",
    "    plot_history=7 * 12,\n",
    "    confidence=80\n",
    "):\n",
    "    print(\"calling served model to generate predictions starting from {}\".format(str(forecast_date)))\n",
    "    assert(confidence > 50 and confidence < 100)\n",
    "    low_quantile = 0.5 - confidence * 0.005\n",
    "    up_quantile = confidence * 0.005 + 0.5\n",
    "        \n",
    "    # we first construct the argument to call our model\n",
    "    args = {\n",
    "        \"ts\": target_ts[:forecast_date],\n",
    "        \"return_samples\": show_samples,\n",
    "        \"quantiles\": [low_quantile, 0.5, up_quantile],\n",
    "        \"num_samples\": 100\n",
    "    }\n",
    "\n",
    "\n",
    "    if dynamic_feat is not None:\n",
    "        args[\"dynamic_feat\"] = dynamic_feat\n",
    "        fig = plt.figure(figsize=(20, 6))\n",
    "        ax = plt.subplot(2, 1, 1)\n",
    "    else:\n",
    "        fig = plt.figure(figsize=(20, 3))\n",
    "        ax = plt.subplot(1,1,1)\n",
    "    \n",
    "    if cat is not None:\n",
    "        args[\"cat\"] = cat\n",
    "        ax.text(0.9, 0.9, 'cat = {}'.format(cat), transform=ax.transAxes)\n",
    "\n",
    "    # call the end point to get the prediction\n",
    "    prediction = predictor.predict(**args)\n",
    "\n",
    "    # plot the samples\n",
    "    if show_samples: \n",
    "        for key in prediction.keys():\n",
    "            if \"sample\" in key:\n",
    "                prediction[key].plot(color='lightskyblue', alpha=0.2, label='_nolegend_')\n",
    "                \n",
    "                \n",
    "    # plot the target\n",
    "    #target_section = target_ts[forecast_date-plot_history:forecast_date+prediction_length]\n",
    "    target_section = target_ts[forecast_date-pd.Timedelta(hours=plot_history):forecast_date+pd.Timedelta(hours=prediction_length)]\n",
    "    target_section.plot(color=\"black\", label='target')\n",
    "    \n",
    "    # plot the confidence interval and the median predicted\n",
    "    ax.fill_between(\n",
    "        prediction[str(low_quantile)].index, \n",
    "        prediction[str(low_quantile)].values, \n",
    "        prediction[str(up_quantile)].values, \n",
    "        color=\"b\", alpha=0.3, label='{}% confidence interval'.format(confidence)\n",
    "    )\n",
    "    prediction[\"0.5\"].plot(color=\"b\", label='P50')\n",
    "    ax.legend(loc=2)    \n",
    "    \n",
    "    # fix the scale as the samples may change it\n",
    "    ax.set_ylim(target_section.min() * 0.5, target_section.max() * 1.5)\n",
    "    \n",
    "    if dynamic_feat is not None:\n",
    "        for i, f in enumerate(dynamic_feat, start=1):\n",
    "            ax = plt.subplot(len(dynamic_feat) * 2, 1, len(dynamic_feat) + i, sharex=ax)\n",
    "            feat_ts = pd.Series(\n",
    "                #index=pd.DatetimeIndex(start=target_ts.index[0], freq=target_ts.index.freq, periods=len(f)),\n",
    "                index = pd.date_range(target_ts.index[0], periods = len(f), freq=target_ts.index.freq),\n",
    "                data=f\n",
    "            )\n",
    "            #feat_ts[forecast_date-plot_history:forecast_date+prediction_length].plot(ax=ax, color='g')\n",
    "            feat_ts[forecast_date-pd.Timedelta(hours=plot_history):forecast_date+pd.Timedelta(hours=prediction_length)].plot(ax=ax, color='g')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can interact with the function previously defined, to look at the forecast of any customer at any point in (future) time. \n",
    "\n",
    "For each request, the predictions are obtained by calling our served model on the fly.\n",
    "\n",
    "Here we forecast the consumption of an office after week-end (note the lower week-end consumption). \n",
    "You can select any time series and any forecast date, just click on `Run Interact` to generate the predictions from our served endpoint and see the plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "style = {'description_width': 'initial'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@interact_manual(\n",
    "    vendor_id=IntSlider(min=0, max=1, value=1, style=style), \n",
    "    forecast_day=IntSlider(min=0, max=100, value=51, style=style),\n",
    "    confidence=IntSlider(min=60, max=95, value=80, step=5, style=style),\n",
    "    history_weeks_plot=IntSlider(min=1, max=20, value=1, style=style),\n",
    "    show_samples=Checkbox(value=False),\n",
    "    continuous_update=False\n",
    ")\n",
    "def plot_interact(vendor_id, forecast_day, confidence, history_weeks_plot, show_samples):\n",
    "    plot(\n",
    "        predictor,\n",
    "        target_ts=timeseries[vendor_id],\n",
    "        forecast_date=end_training + datetime.timedelta(days=forecast_day),\n",
    "        show_samples=show_samples,\n",
    "        plot_history=history_weeks_plot * 12 * 7,\n",
    "        confidence=confidence\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delete endpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### added by Roop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done waiting to finish Data API.\n"
     ]
    }
   ],
   "source": [
    "query_str = \"\"\"SELECT quantity,store_hk,\n",
    "       category_hk,\n",
    "       item_hk,\n",
    "       item_iscorporate,\n",
    "       item_isstocked,\n",
    "       item_includeai,\n",
    "       item_isdepartmentitem,\n",
    "       item_isedi,\n",
    "       item_ispriceunitonlabel,\n",
    "       item_issendtomorgue,\n",
    "       item_isinactive,\n",
    "       item_isconverted,\n",
    "       item_isdiscontinued,\n",
    "       item_isincludeexo,\n",
    "       item_isbasketactive,\n",
    "       item_islocalitemreturnloss,\n",
    "       category_iscosmetic,\n",
    "       category_isotc,\n",
    "       category_isrx,\n",
    "       category_iscoupon,\n",
    "       transaction_year,\n",
    "       transaction_quarter,\n",
    "       transaction_month,\n",
    "       transaction_month_week,\n",
    "       transaction_week_day,\n",
    "       holiday\n",
    "FROM public.product_sales WHERE sl <10\"\"\"\n",
    "\n",
    "res = client_redshift.execute_statement(Database= db, SecretArn= secret_arn, Sql= query_str, ClusterIdentifier= cluster_id)\n",
    "\n",
    "id = res[\"Id\"]\n",
    "# Waiter in try block and wait for DATA API to return\n",
    "try:\n",
    "    custom_waiter.wait(Id=id)\n",
    "    print(\"Done waiting to finish Data API.\")\n",
    "except WaiterError as e:\n",
    "    print (e)\n",
    "\n",
    "output=client_redshift.get_statement_result(Id=id)\n",
    "nrows=output[\"TotalNumRows\"]\n",
    "ncols=len(output[\"ColumnMetadata\"])\n",
    "#print(\"Number of columns: %d\" %ncols)\n",
    "resultrows=output[\"Records\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unload ('select * from venue')\n",
    "to 's3://mybucket/tickit/unload/venue_' \n",
    "iam_role 'arn:aws:iam::0123456789012:role/MyRedshiftRole'\n",
    "parallel off;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "quantity                      9\n",
       "store_hk                      9\n",
       "category_hk                   9\n",
       "item_hk                       9\n",
       "item_iscorporate              9\n",
       "item_isstocked                9\n",
       "item_includeai                9\n",
       "item_isdepartmentitem         9\n",
       "item_isedi                    9\n",
       "item_ispriceunitonlabel       9\n",
       "item_issendtomorgue           9\n",
       "item_isinactive               9\n",
       "item_isconverted              9\n",
       "item_isdiscontinued           9\n",
       "item_isincludeexo             9\n",
       "item_isbasketactive           9\n",
       "item_islocalitemreturnloss    9\n",
       "category_iscosmetic           9\n",
       "category_isotc                9\n",
       "category_isrx                 9\n",
       "category_iscoupon             9\n",
       "transaction_year              9\n",
       "transaction_quarter           9\n",
       "transaction_month             9\n",
       "transaction_month_week        9\n",
       "transaction_week_day          9\n",
       "holiday                       9\n",
       "dtype: int64"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = df.sample(frac=0.8,random_state=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = df.drop(train_data.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_no_target = test_data.drop(columns=['quantity'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "region = boto3.Session().region_name\n",
    "\n",
    "session = sagemaker.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket = session.default_bucket()\n",
    "prefix = 'sagemaker/public'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data uploaded to: s3://sagemaker-us-east-1-001876746742/sagemaker/public/train/train_data.csv\n",
      "Test data uploaded to: s3://sagemaker-us-east-1-001876746742/sagemaker/public/test/test_data.csv\n"
     ]
    }
   ],
   "source": [
    "train_file = 'train_data.csv';\n",
    "train_data.to_csv(train_file, index=False, header=True)\n",
    "train_data_s3_path = session.upload_data(path=train_file, key_prefix=prefix + \"/train\")\n",
    "print('Train data uploaded to: ' + train_data_s3_path)\n",
    "\n",
    "test_file = 'test_data.csv';\n",
    "test_data_no_target.to_csv(test_file, index=False, header=False)\n",
    "test_data_s3_path = session.upload_data(path=test_file, key_prefix=prefix + \"/test\")\n",
    "print('Test data uploaded to: ' + test_data_s3_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data_config = [{\n",
    "      'DataSource': {\n",
    "        'S3DataSource': {\n",
    "          'S3DataType': 'S3Prefix',\n",
    "          'S3Uri': 's3://{}/{}/train'.format(bucket,prefix)\n",
    "        }\n",
    "      },\n",
    "      'TargetAttributeName': 'quantity'\n",
    "    }\n",
    "  ]\n",
    "\n",
    "output_data_config = {\n",
    "    'S3OutputPath': 's3://{}/{}/output'.format(bucket,prefix)\n",
    "  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker import get_execution_role\n",
    "role = get_execution_role()\n",
    "# This is the client we will use to interact with SageMaker AutoPilot\n",
    "sm = boto3.Session().client(service_name='sagemaker',region_name=region)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AutoMLJobName: automl-public-29-00-48-43\n"
     ]
    },
    {
     "ename": "ClientError",
     "evalue": "An error occurred (ValidationException) when calling the CreateAutoMLJob operation: Dataset is not large enough: expected minimum number of rows is 500 but only 7 were found.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mClientError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-c9d49045f67e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m                                        \u001b[0;34m{\u001b[0m\u001b[0;34m'MaxCandidates'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m                                       },\n\u001b[0;32m---> 13\u001b[0;31m                       RoleArn=role)\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/botocore/client.py\u001b[0m in \u001b[0;36m_api_call\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    355\u001b[0m                     \"%s() only accepts keyword arguments.\" % py_operation_name)\n\u001b[1;32m    356\u001b[0m             \u001b[0;31m# The \"self\" in this scope is referring to the BaseClient.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 357\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_api_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moperation_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    358\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m         \u001b[0m_api_call\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpy_operation_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/botocore/client.py\u001b[0m in \u001b[0;36m_make_api_call\u001b[0;34m(self, operation_name, api_params)\u001b[0m\n\u001b[1;32m    674\u001b[0m             \u001b[0merror_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsed_response\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Error\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Code\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    675\u001b[0m             \u001b[0merror_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_code\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 676\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0merror_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparsed_response\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moperation_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    677\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    678\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mparsed_response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mClientError\u001b[0m: An error occurred (ValidationException) when calling the CreateAutoMLJob operation: Dataset is not large enough: expected minimum number of rows is 500 but only 7 were found."
     ]
    }
   ],
   "source": [
    "from time import gmtime, strftime, sleep\n",
    "timestamp_suffix = strftime('%d-%H-%M-%S', gmtime())\n",
    "\n",
    "auto_ml_job_name = 'automl-public-' + timestamp_suffix\n",
    "print('AutoMLJobName: ' + auto_ml_job_name)\n",
    "\n",
    "sm.create_auto_ml_job(AutoMLJobName=auto_ml_job_name,\n",
    "                      InputDataConfig=input_data_config,\n",
    "                      OutputDataConfig=output_data_config,\n",
    "                      AutoMLJobConfig={'CompletionCriteria':\n",
    "                                       {'MaxCandidates': 20}\n",
    "                                      },\n",
    "                      RoleArn=role)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## end of roop code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
